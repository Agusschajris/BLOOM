{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5,
  "cells": [
    {
      "id": "0notebook_explanation",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Jupyter Notebooks\n",
        "Esto es un notebook de Jupyter, puedes ejecutar cada celda de código presionando el botón de play que aparece al lado de cada celda, o usar la opción de ejecutar todo.\n",
        "Los notebooks de Jupyter son una herramienta muy útil para experimentar con código, visualizar datos y compartir resultados. Son muy populares en el mundo de la ciencia de datos y el machine learning."
      ]
    },
    {
      "id": "1package_install_explanation",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "### Instalación de paquetes\n",
        "Primero, necesitamos instalar las librerías a usar en nuestro código.\n",
        "[TensorFlow](https://www.tensorflow.org/), que es la librería que usaremos para entrenar el modelo de machine learning.\n",
        "Y la herramienta de datasets de [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/).\n",
        "Otros paquetes necesarios para el entrenamiento del modelo se instalarán automáticamente como dependencias; por ejemplo, la librería utilizada para leer datasets, [pandas](https://pandas.pydata.org/).\n",
        "Para instalar estos paquetes, ejecuta la siguiente celda de código."
      ]
    },
    {
      "id": "2package_install",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "!pip install tensorflowjs ucimlrepo"
    },
    {
      "id": "3library_import_explanation",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Importación de librerías\n",
        "Ahora, importaremos las librerías que usaremos en nuestro código.\n",
        "Para importar las librerías, ejecuta la siguiente celda de código."
      ]
    },
    {
      "id": "4library_import",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflowjs as tfjs\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import drive\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pandas as pd"
      ]
    },
    {
      "id": "5dataset_import_explanation",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Importación de Dataset\n",
        "Ahora, importaremos el dataset que usaremos para entrenar el modelo.\n",
        "Usaremos la función `fetch_ucirepo` para importar el dataset.\n",
        "En este caso, el dataset que usaremos es [",
        "Esta herramienta nos facilita la importación de los datasets, procesando los datos y devolviendo un DataFrame de pandas, sin tener que depurarlo nosotros a mano.\n",
        "Esto está bien para empezar, pero en un proyecto real, es más complejo, debiendo entender la estructura y el formato de los datos.\n",
        "Para importar el dataset, ejecuta la siguiente celda de código."
      ]
    },
    {
      "id": "6dataset_import",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "dataset = fetch_ucirepo(id=",
        "print(dataset.metadata)\n",
        "display(dataset.data.features)\n",
        "display(dataset.data.targets)"
      ]
    },
    {
      "id": "7missing_values_explanation",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Valores faltantes\n",
        "En este caso, nuestro dataset tiene valores faltantes.\n",
        "Esto es un problema, ya que los modelos de machine learning no pueden manejar valores faltantes.\n",
        "Para solucionar este problema, eliminaremos las filas con valores faltantes.\n",
        "Para eliminar las filas con valores faltantes, ejecuta la siguiente celda de código."
      ]
    },
    {
      "id": "8missing_values",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Contar valores faltantes antes.\n",
        "print('Antes: ', dataset.data.features.isna().sum().sum() + dataset.data.targets.isna().sum().sum())\n",
        "\n",
        "# Ahora limpiamos el dataset.\n",
        "dataset.data.features = dataset.data.features.dropna()\n",
        "dataset.data.targets = dataset.data.targets.dropna()\n",
        "\n",
        "# Contar valores faltantes después.\n",
        "print('Después: ', dataset.data.features.isna().sum().sum() + dataset.data.targets.isna().sum().sum())"
      ]
    },
    {
      "id": "9categorical_explanation",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Variables categóricas\n",
        "En este caso, nuestro dataset tiene variables categóricas.\n",
        "Esto es un problema, ya que los modelos de machine learning no pueden manejar variables de texto, solo números.\n",
        "Para solucionar este problema, convertiremos las variables categóricas en variables numéricas, asignandole un número a cada categoría.\n",
        "Para convertir las variables categóricas en variables numéricas, ejecuta la siguiente celda de código."
      ]
    },
    {
      "id": "10categorical",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Lista con las asignaciones de las variables categóricas.\n",
        "label_encoders = {}\n",
        "for column in dataset.data.features.columns:\n",
        "    if dataset.data.features[column].dtype == 'object':  # Fijarse si la columna es categórica.\n",
        "        # Crear un codificador para asignar números a las categorías.\n",
        "        le = LabelEncoder()\n",
        "        dataset.data.features[column] = le.fit_transform(dataset.data.features[column])\n",
        "        label_encoders[column] = le\n",
        "\n",
        "        # Aplicar el mismo codificador a los datos.\n",
        "        dataset.data.features[column] = le.transform(dataset.data.features[column])\n",
        "\n",
        "for column in dataset.data.targets.columns:\n",
        "    if dataset.data.targets[column].dtype == 'object':  # Fijarse si la columna es categórica.\n",
        "        # Crear un codificador para asignar números a las categorías.\n",
        "        le = LabelEncoder()\n",
        "        dataset.data.targets[column] = le.fit_transform(dataset.data.targets[column])\n",
        "        label_encoders[column] = le\n",
        "\n",
        "        # Aplicar el mismo codificador a los datos.\n",
        "        dataset.data.targets[column] = le.transform(dataset.data.targets[column])\n",
        "\n",
        "display(dataset.data.features)\n",
        "display(dataset.data.targets)"
      ]
    },
    {
      "id": "11data_split_explanation",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### División de datos\n",
        "Para entrenar un modelo de machine learning, necesitamos dividir los datos en dos conjuntos: uno para entrenar el modelo y otro para evaluarlo.\n",
        "Usaremos el 80% de los datos para entrenar el modelo y el 20% restante para evaluarlo.\n",
        "Para dividir los datos, ejecuta la siguiente celda de código."
      ]
    },
    {
      "id": "12data_split",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
        "source": [
          "X_train, X_test, y_train, y_test = train_test_split(dataset.data.features, dataset.data.targets, test_size=0.2)\n",
          "display(X_train)\n",
          "display(y_train)\n",
          "display(X_test)\n",
          "display(y_test)"
        ]
    },
    {
      "id": "13drive_mount_explanation",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Acceder al modelo\n",
        "\n",
        "### Montar Google Drive\n",
        "Para cargar el esquema del modelo, y luego guardar el modelo entrenado y los resultados, necesitamos montar Google Drive.\n",
        "Para montar Google Drive, ejecuta la siguiente celda de código."
      ]
    },
    {
      "id": "14drive_mount",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "drive.mount('/content/drive')"
    },
    {
      "id": "15model_import_explanation",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cargar el modelo",
        "Ahora sí, cargaremos el esquema del modelo que usaremos para entrenar.\n",
        "Para cargar el esquema del modelo, ejecuta la siguiente celda de código."
      ]
    },
    {
      "id": "16model_import",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "model = tfjs.converters.load_keras_model('/content/drive/My Drive/Bloom/"
    },
    {
      "id": "17model_train_explanation",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Entrenar el modelo\n",
        "Ahora, entrenaremos el modelo con los datos de entrenamiento, pasándole los datos que le habíamos separado.\n",
        "Para entrenar el modelo, ejecuta la siguiente celda de código."
      ]
    },
    {
      "id": "18model_train",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "model.fit(X_train, y_train, epochs=10, verbose=2, validation_data=(X_test, y_test))"
      ]
    },
    {
      "id": "19epochs_explanation",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
          "### Mientras se entrena: Epochs\n",
          "Un epoch es una pasada completa de todos los datos de entrenamiento a través del modelo.\n",
          "En cada epoch, el modelo ajusta sus pesos para minimizar la función de pérdida.\n",
          "El número de epochs es un hiperparámetro que debemos ajustar.\n",
          "Si el número de epochs es muy bajo, el modelo no tendrá suficiente tiempo para aprender.\n",
          "Si el número de epochs es muy alto, el modelo puede sobreajustarse a los datos de entrenamiento.\n",
          "Puedes jugar cambiando el número de epochs y ver cómo afecta al modelo!"
      ]
    },
    {
      "id": "20model_analysis_explanation",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
          "## Analizar el modelo\n",
          "Ahora, analizaremos el modelo entrenado.\n",
          "Para analizar el modelo, ejecuta la siguiente celda de código."
      ]
    },
    {
      "id": "21model_analysis",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "model.summary()"
    },
    {
      "id": "22model_predict_explanation",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Predecir con el modelo\n",
        "Finalmente, usaremos el modelo entrenado para hacer predicciones con los datos de prueba.\n",
        "Para hacer predicciones con el modelo, rellena los valores en el código y ejecuta la siguiente la celda."
      ]
    },
    {
      "id": "23model_predict",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "predict_data = pd.DataFrame({\n"
      ]
    },
    {
      "id": "24model_save_explanation",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Guardar el modelo\n",
        "Finalmente, guardaremos el modelo entrenado en Google Drive.\n",
        "Para guardar el modelo, ejecuta la siguiente celda de código."
      ]
    },
    {
      "id": "25model_save",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "tfjs.converters.save_keras_model(model, '/content/drive/My Drive/Bloom/"
    }
  ]
}